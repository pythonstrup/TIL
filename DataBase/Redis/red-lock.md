# 레디스의 분산 락(RedLock)

## redis Locking

- 레디스는 싱글 스레드로 동작하기 때문에 동시성 문제가 발생하지 않는다.
  - 따라서 리소스에 대해 값을 설정하여, 값이 설정된 경우에는 다른 리소스의 접근을 차단할 수 있다. -> 잠금 
- 만약 단일 노드만 사용한다면 아래와 같은 명령어만으로도 충분할 수 있다.

```redis
// NotExists일 때만 저장. 만료 시간을 5초 동안 유지.
> SET key value NX PX 5000
```

- NotExists일 때만 저장하고 만료시간을 5초로 지정한다? 이게 무슨 의미일까?
  - 여기서 `NX`는 락이 걸려 있는지 확인하는 옵션으로 사용되고, `PX`는 락의 시간으로 사용된다.
- 또한 락을 해제할 때는 키를 삭제하듯 `DEL` 연산으로 키를 삭제하면 된다.

```redis
> DEL key
```

- 하지만 여기서 문제가 발생할 수 있다.
  - 클라이언트 A가 만든 락을 클라이언트 B가 `DEL` 연산을 통해 해제해버리고 획득하게 되어버리면?

### Lua Script

- 위와 같은 문제를 방지하기 위해 루아 스크립트를 실행시켜 락을 해제할 수도 있다.

```lua
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

- 이 스크립트는 Redis에게 "키가 존재하고, 그 값이 내가 기대하는 값과 정확히 일치할 때만 삭제하라."고 지시한다.
- 다른 클라이언트가 만든 락을 잘못 삭제하는 문제를 방지하기 위해서다.
- "서명"된 고유한 랜덤 문자열을 붙여 해당 클라이언트가 만든 락일 때만 삭제되도록 해야 한다.

### 랜덤 문자열은 어떤 걸 써야 할까?

- 문서에서는 다음을 추천한다.
1. `/dev/urandom`에서 20바이트를 추출해서 사용하는 방식. (비교적 비용이 높은 방식)
2. `/dev/urandom`을 시드로 해서 RC4로 의사난수 스트림 생성 (저렴한 방식)
3. 마이크로초 단위 타임스탬프 + 클라이언트 ID를 조합한 문자열 (간단한 방식. 절대적으로 안전하진 않지만, 대부분의 환경에선 충분히 안전함.)

### 여기서 문제! Failover만으로 충분하지 않은 이유.

- 그러면 위와 같은 단순한 명령어만 활용하면 분산 락을 사용하기에 충분할까? 그렇지 않다. 
- Replication을 사용할 경우, 마스터 노드가 새로 승격되는 과정에서 race condition이 발생할 수 있다.
1. 클라이언트 A가 Master 노드에서 잠금을 획득한다.
2. 키에 대한 쓰기가 복제본으로 전송되기 전에 마스터가 다운된다.
3. 복제본이 마스터로 승격된다.
4. 클라이언트 B는 A가 잠금을 보유하고 있는 동일한 리소스에 대한 잠금을 획득한다.

---

## 분산 락의 필요성

- 분산 환경에서 서로 다른 클라이언트가 공유 자원에 대해 `상호 배타적`으로 작업해야 할 때 유용.
  - 단순한 Locking만으로는 race condition이 발생할 수 있으므로 다른 방법이 필요함.
- Redis는 분산 락을 구현하기 위한 표준적인 알고리즘이 있다.
  - `RedLock` 알고리즘

### 안전성과 활성성의 보장

- 분산 락을 효과적으로 사용하기 위해 최소한 3가지 속성 보장되어야 한다.

#### 1. Safety property

- 어떤 시점에도 오직 하나의 클라이언트만 락을 보유할 수 있어야 한다.

#### 2. Liveness property A: 데드락 방지(Deadlock free)

- 리소스를 Lock 잡은 클라이언트가 크래시되거나 네트워크 분할이 발생하더라도 결국에는 언제나 락을 획득할 수 있어야 한다.

#### 3. Liveness property B: 장애 허용(Fault tolerance)

- Redis 노드 중 과반수가 살아있다면, 클라이언트는 락을 획득하고 해제할 수 있어야 한다.

---

## `RedLock` 알고리즘

- N개의 레디스 인스턴스가 있다고 가정해보자.
  - 이 노드들은 완전히 독립적이며, 복제나 어떤 형태의 암묵적 동기화 메커니즘도 사용하지 않는다.
- N = 5라고 가정하고 알고리즘 설명.
- 클라이언트는 분산 환경에서 락을 획득하기 위해 다음 작업을 수행한다.
1. 현재 시간을 ms 단위로 가져온다.
2. N개의 Redis 인스턴스에 동일한 key와 random value로 잠금을 획득하려고 시도한다. 
   - 순차적으로 모든 인스턴스에 락을 걸려고 시도한다.
   - `SET <key> <value> NX PX <ttl>`
   - 각 인스턴스에 잠금을 설정할 때 클라이언트는 **전체 잠금 자동 해제 시간에 비해 작은 타임아웃을 사용하여 잠금을 획득**한다.
     - ex) 자동 해제 시간이 10s인 경우, 타임아웃은 5~50ms가 될 수 있다. 
   - 장애가 발생한 Redis에 너무 오래 블로킹되는 것을 방지할 수 있다. (빠르게 다음 인스턴스로 넘어갈 수 있다.)
3. 전체 락 획득에 걸린 시간을 측정한다.
   - 클라이언트는 `{현재 시간} - {1단계에서 얻은 타임스탬프}`로 잠금을 획득하기 위해 경과한 시간을 계산한다.
   - 아래 조건을 모두 만족해야 획득한 것으로 간주된다.
     - 클라이언트가 과반이 넘는 인스턴스에서 잠금을 획득했고! 
     - `총 경과 시간`이 `잠금 유효 시간(TTL)`보다 적다면!
   - ex1) N=5 -> 최소 3개 인스턴스에서 락을 획득해야 한다.
   - ex2) TTL인 10초를 초과했다면 실패로 간주한다.
4. 락 획득에 성공했다면?
   - 최종적인 락 유효 시간은 처음 설정한 TTL에서 `걸린 시간 elapsed time`을 뺀 값으로 간주한다.
5. 락 획득 실패 시 (만약 Quorum만큼의 인스턴스에서 락을 잡지 못했거나, TTL을 초과했다면)
   - 락을 획득하지 못한 것으로 간주하고, 락을 시도했던 모든 Redis 인스턴스에서 락을 해제 시도한다.
   - 락을 못 잡았다고 판단한 인스턴스에도 시도한다.

### clock drift?

- 실제 컴퓨터 환경은 자체적인 시계를 가지고 있기 때문에 서로 간의 `시계 오차 clock drift`가 발생할 수 있다.
- `RedLock`은  각 프로세스의 로컬 시간은 락의 `자동 해제 시간(auto-release time)`에 비해 비교적 일정한 속도로 흐른다는 가정에 기반한다.
  - `clock drift`가 대부분 매우 작다고 보는 것이다.
- 락을 보유한 클라이언트가 작업을 TTL 내에 종료하는 경우에만 상호 배타성이 보장되는 문제가 발생할 수 있다.
  - 따라서 프로세스 간 `clock drift`를 보정하기 위해 몇 밀리초 정도 여유 시간을 빼주는 것이 필요하다.

### Retry

- 락을 획득하지 못했다면, 랜덤한 지연 시간 이후 다시 시도해야 한다.
  - 여러 클라이언트가 동시에 동일한 리소스에 대해 락을 시도할 경우 발생할 수 있는 `split brain` 상황을 완화하기 위해서다.
- 이상적으로는 N개의 Redis 인스턴스에 동시에 SET 명령을 보내도록 멀티플렉싱을 사용해야 한다.
- 또한, 락 획득에 실패한 클라이언트는, 락이 부분적으로 걸린 인스턴스들을 빠르게 해제해야 한다.
  - 그래야 다른 클라이언트가 키 만료를 기다리지 않고 빠르게 락을 재시도할 수 있다.

---

# 참고 자료

- [Redis 공식 문서](https://redis.io/docs/latest/develop/use/patterns/distributed-locks/)
- [[Redis] 레디스가 제공하는 분산락(RedLock)의 특징과 한계](https://mangkyu.tistory.com/311)
